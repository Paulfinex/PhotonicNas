{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "78afd832-bbb2-4c8a-bc7a-8c83c21d9fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import tensorboard\n",
    "import tensorboardX\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nni\n",
    "from nni.nas.evaluator.pytorch import Lightning, ClassificationModule, Trainer\n",
    "from nni.nas.experiment import NasExperiment\n",
    "from nni.nas.space import model_context\n",
    "from nni.nas.hub.pytorch import DARTS\n",
    "from nni.nas.strategy import DARTS as DartsStrategy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from nni.nas.experiment import NasExperiment\n",
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "import nni.nas.strategy as strategy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from ops import AvgPool,DilConv,SepConv\n",
    "import genotypes\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87026b-13df-4a27-a2a7-6c511c8feaeb",
   "metadata": {},
   "source": [
    "## Auxiliary Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "151bd8cd-8689-477f-8e09-4d0ff24ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nni.trace\n",
    "class AuxLossClassificationModule(ClassificationModule):\n",
    "    \"\"\"Several customization for the training of DARTS, based on default Classification.\"\"\"\n",
    "    model: DARTS\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 0.001,\n",
    "                 weight_decay: float = 0.,\n",
    "                 auxiliary_loss_weight: float = 0.4,\n",
    "                 max_epochs: int = 600):\n",
    "        self.auxiliary_loss_weight = auxiliary_loss_weight\n",
    "        self.max_epochs = max_epochs\n",
    "        super().__init__(learning_rate=learning_rate, weight_decay=weight_decay, num_classes=10)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Customized optimizer with momentum, as well as a scheduler.\"\"\"\n",
    "        \"\"\"\n",
    "        Classification Module params from nni.nas.evaluators\n",
    "             learning_rate: float\n",
    "             weight_decay: float\n",
    "             optimizer: Type[optim.Optimizer]\n",
    "             export_onnx: bool \n",
    "             num_classes: Optional[int] \n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr = 0.001,\n",
    "            betas=(0.9, 0.999),  # type: ignore\n",
    "            eps=1e-07,\n",
    "            weight_decay= self.auxiliary_loss_weight  # type: ignore\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.max_epochs, eta_min=1e-3)\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step, customized with auxiliary loss.\"\"\"\n",
    "        x, y = batch\n",
    "\n",
    "        # Check for NaNs or infinite values in input\n",
    "        if torch.isnan(x).any() or torch.isnan(y).any() or torch.isinf(x).any() or torch.isinf(y).any():\n",
    "            raise ValueError(\"Input data contains NaNs or Infinities.\")\n",
    "\n",
    "        if self.auxiliary_loss_weight:\n",
    "            y_hat, y_aux = self(x)\n",
    "            loss_main = self.criterion(y_hat, y)\n",
    "            loss_aux = self.criterion(y_aux, y)\n",
    "            # Check for NaNs in loss values\n",
    "            if torch.isnan(loss_main).any() or torch.isnan(loss_aux).any():\n",
    "                raise ValueError(\"Loss contains NaNs.\")\n",
    "            self.log('train_loss_main', loss_main)\n",
    "            self.log('train_loss_aux', loss_aux)\n",
    "            loss = loss_main + self.auxiliary_loss_weight * loss_aux\n",
    "\n",
    "        else:\n",
    "            y_hat = self(x)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        for name, metric in self.metrics.items():\n",
    "            self.log('train_' + name, metric(y_hat, y), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Set drop path probability before every epoch. This has no effect if drop path is not enabled in model.\"\"\"\n",
    "        self.model.set_drop_path_prob(self.model.drop_path_prob * self.current_epoch / self.max_epochs)\n",
    "\n",
    "        # Logging learning rate at the beginning of every epoch\n",
    "        self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13293a-09d0-44b0-aa12-60b4ddfb1803",
   "metadata": {},
   "source": [
    "## Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95620af6-1cbc-4d76-9834-060c5bc4e8be",
   "metadata": {},
   "source": [
    "### Random cutout transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9c05a3f3-a6c6-4436-894f-7c2d4d28155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout_transform(img, length: int = 16):\n",
    "    h, w = img.size(1), img.size(2)\n",
    "    mask = np.ones((h, w), np.float32)\n",
    "    y = np.random.randint(h)\n",
    "    x = np.random.randint(w)\n",
    "\n",
    "    y1 = np.clip(y - length // 2, 0, h)\n",
    "    y2 = np.clip(y + length // 2, 0, h)\n",
    "    x1 = np.clip(x - length // 2, 0, w)\n",
    "    x2 = np.clip(x + length // 2, 0, w)\n",
    "\n",
    "    mask[y1: y2, x1: x2] = 0.\n",
    "    mask = torch.from_numpy(mask)\n",
    "    mask = mask.expand_as(img)\n",
    "    img *= mask\n",
    "    return img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaab426-d3a4-4b30-a3d5-f73f475144c1",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1aad9ec9-8c31-430c-a2cd-8cd40840178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cifar10_dataset(train: bool = True, cutout: bool = False):\n",
    "    CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "    CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "    if train:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "        ])\n",
    "        if cutout:\n",
    "            transform.transforms.append(cutout_transform)\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "        ])\n",
    "\n",
    "    return nni.trace(CIFAR10)(root='./data', train=train, download=True, transform=transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa4465-8848-4d50-bdae-728e3d4a99c2",
   "metadata": {},
   "source": [
    "## Darts Model Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697405a-8810-42c6-b67a-893dcdb68c8a",
   "metadata": {},
   "source": [
    "### Model Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e04ab7a-207c-46e1-b17b-dfb1ef28b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.nas.nn.pytorch import LayerChoice, ModelSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77105c95-c2f2-4d62-84ba-47c5b02975c1",
   "metadata": {},
   "source": [
    "### Model Kernel 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e4fdc7a-4af0-4448-b670-a5086ef78f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDARTSSpaceK2(ModelSpace):\n",
    "    def __init__(self, input_channels, channels, num_classes, layers):\n",
    "        super(CustomDARTSSpaceK2, self).__init__()\n",
    "        #self.first_iter = True\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.drop_path_prob = 0.0  \n",
    "        self.preliminary_layer = nn.Conv2d(3, 8, kernel_size=2, padding=0, bias=False)\n",
    "        \n",
    "        layer1 = LayerChoice([\n",
    "            SepConv(8, 16, kernel_size=2, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=1, padding=0),\n",
    "                nn.Conv2d(8, 16, kernel_size=1)\n",
    "            )        \n",
    "        ], label='layer_1')\n",
    "        self.layers.append(layer1)\n",
    "        \n",
    "        layer2 = LayerChoice([\n",
    "            SepConv(16, 20, kernel_size=2, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=1, padding=0),\n",
    "                nn.Conv2d(16, 20, kernel_size=1)\n",
    "            )        \n",
    "        ], label='layer_2')\n",
    "        self.layers.append(layer2)\n",
    "        \n",
    "        layer3 = LayerChoice([\n",
    "            SepConv(20, 24, kernel_size=2, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=1, padding=0),\n",
    "                nn.Conv2d(20, 24, kernel_size=1)\n",
    "            )             \n",
    "        ], label='layer_3')\n",
    "        self.layers.append(layer3)\n",
    "        \n",
    "        layer4 = LayerChoice([\n",
    "            SepConv(24, 32, kernel_size=2, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=1, padding=0),\n",
    "                nn.Conv2d(24, 32, kernel_size=1)\n",
    "            )            \n",
    "        ], label='layer_4')\n",
    "        self.layers.append(layer4)\n",
    "        \n",
    "        layer5 = LayerChoice([\n",
    "            SepConv(32, 40, kernel_size=2, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=1, padding=0),\n",
    "                nn.Conv2d(32, 40, kernel_size=1)\n",
    "            )              \n",
    "        ], label='layer_5')\n",
    "        self.layers.append(layer5)\n",
    "        \n",
    "        # Ensure the number of inputs to fc1 is close to but not exceeding 200\n",
    "        self.pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(160, 96)  # Adjust input dimensions according to final channels and output dimensions\n",
    "        self.fc2 = nn.Linear(96, 64)  # Adjust input dimensions according to final channels and output dimensions\n",
    "        self.fc3 = nn.Linear(64, 32)  # Adjust input dimensions according to final channels and output dimensions\n",
    "        \n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Kernel: 2\n",
    "        # Initial shape: 32x32\n",
    "        #if first_iter:print(f'Input shape: {x.shape}')\n",
    "        x = self.preliminary_layer(x)\n",
    "        # After preliminary layer: 31x31\n",
    "        #if first_iter:print(f'After preliminary layer: {x.shape}')\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            #if first_iter:print(f'After layer {i+1}: {x.shape}')\n",
    "            \n",
    "            # Add a AvgPool2d layer after the third layer to reduce spatial dimensions\n",
    "            if i == 2 or i == 4:\n",
    "                x = nn.AvgPool2d(kernel_size=2, stride=2)(x)\n",
    "                #if first_iter:print(f'After avg pooling: {x.shape}')\n",
    "        \n",
    "        # Add an adaptive pooling layer before flattening\n",
    "        x = self.pool(x)\n",
    "        #if first_iter:print(f'After adaptive pooling: {x.shape}')\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        #if first_iter:print(f'After flattening: {x.shape}')\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        #if first_iter:print(f'After fc1: {x.shape}')\n",
    "        x = self.fc2(x)\n",
    "        #if first_iter:print(f'After fc2: {x.shape}')\n",
    "        x = self.fc3(x)\n",
    "        #if first_iter:print(f'After fc3: {x.shape}')\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        #if first_iter:print(f'After classifier: {x.shape}')\n",
    "        #self.first_iter = False\n",
    "        return x\n",
    "\n",
    "    def set_drop_path_prob(self, drop_path_prob):\n",
    "        self.drop_path_prob = drop_path_prob\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'set_drop_path_prob'):\n",
    "                layer.set_drop_path_prob(drop_path_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949bf98c-f9ff-428e-bc74-4f86e3de0e1d",
   "metadata": {},
   "source": [
    "### Model kernel 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f965761-9a65-42d8-be50-e62a164827f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDARTSSpaceK3(ModelSpace):\n",
    "    def __init__(self, input_channels, channels, num_classes, layers):\n",
    "        super(CustomDARTSSpaceK3, self).__init__()\n",
    "        #self.first_iter = True\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.drop_path_prob = 0.0  \n",
    "        self.preliminary_layer = nn.Conv2d(3, 8, kernel_size=3, padding=0, bias=False)\n",
    "\n",
    "        layer1 = LayerChoice([\n",
    "            SepConv(8, 16, kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(8, 16, kernel_size=1)\n",
    "            )        \n",
    "        ], label='layer_1')\n",
    "        self.layers.append(layer1)\n",
    "        layer2 = LayerChoice([\n",
    "            SepConv(16, 20, kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(16, 20, kernel_size=1)\n",
    "            )        \n",
    "        ], label='layer_2')\n",
    "        self.layers.append(layer2)\n",
    "        \n",
    "        layer3 = LayerChoice([\n",
    "            SepConv(20, 24, kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(20, 24, kernel_size=1)\n",
    "            )             \n",
    "        ], label='layer_3')\n",
    "        self.layers.append(layer3)\n",
    "        \n",
    "        layer4 = LayerChoice([\n",
    "            SepConv(24, 32, kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(24, 32, kernel_size=1)\n",
    "            )            \n",
    "        ], label='layer_4')\n",
    "        self.layers.append(layer4)\n",
    "        \n",
    "        layer5 = LayerChoice([\n",
    "            SepConv(32, 40, kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(32, 40, kernel_size=1)\n",
    "            )              \n",
    "        ], label='layer_5')\n",
    "        self.layers.append(layer5)\n",
    "        \n",
    "        # Ensure the number of inputs to fc1 is close to but not exceeding 200\n",
    "        self.pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(160, 96)  # Adjust input dimensions according to final channels and output dimensions\n",
    "        self.fc2 = nn.Linear(96, 64)  # Adjust input dimensions according to final channels and output dimensions\n",
    "        self.fc3 = nn.Linear(64, 32)  # Adjust input dimensions according to final channels and output dimensions\n",
    "        \n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Kernel: 2\n",
    "        # Initial shape: 32x32\n",
    "        #print(f'Input shape: {x.shape}')\n",
    "        x = self.preliminary_layer(x)\n",
    "        # After preliminary layer: 31x31\n",
    "        #print(f'After preliminary layer: {x.shape}')\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            #print(f'After layer {i+1}: {x.shape}')\n",
    "            \n",
    "            # Add a AvgPool2d layer after the third layer to reduce spatial dimensions\n",
    "            if i == 2 or i == 4:\n",
    "                x = nn.AvgPool2d(kernel_size=2, stride=2)(x)\n",
    "                #print(f'After avg pooling: {x.shape}')\n",
    "        \n",
    "\n",
    "        x =  nn.AvgPool2d(kernel_size=2, stride =2)(x)\n",
    "        #print(f'After adaptive pooling: {x.shape}')\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        #print(f'After flattening: {x.shape}')\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        #print(f'After fc1: {x.shape}')\n",
    "        x = self.fc2(x)\n",
    "        #print(f'After fc2: {x.shape}')\n",
    "        x = self.fc3(x)\n",
    "        #print(f'After fc3: {x.shape}')\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        #print(f'After classifier: {x.shape}')\n",
    "        #self.first_iter = False\n",
    "        return x\n",
    "\n",
    "    def set_drop_path_prob(self, drop_path_prob):\n",
    "        self.drop_path_prob = drop_path_prob\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'set_drop_path_prob'):\n",
    "                layer.set_drop_path_prob(drop_path_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70d395-b4c9-45ec-acf2-3dcc71ba445d",
   "metadata": {},
   "source": [
    "### Darts Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "03e050d4-9227-44fb-ad3c-9149da6f8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(log_dir: str, batch_size: int = 64):\n",
    "    \"\"\"\n",
    "    Darts search \n",
    "\n",
    "    Args:\n",
    "        log_dir (str): The directory where logs will be saved.\n",
    "        batch_size (int, optional): The size of the batches. Default is 64.  \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    model_space =CustomDARTSSpaceK3(input_channels=3, channels=64, num_classes=10, layers=3)\n",
    "    model_space.set_drop_path_prob(0.2)\n",
    "    train_data = get_cifar10_dataset()\n",
    "    num_samples = len(train_data)\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    split = num_samples // 2\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=batch_size,\n",
    "        sampler=SubsetRandomSampler(indices[:split]),\n",
    "        pin_memory=True, num_workers=6,persistent_workers=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        train_data, batch_size=batch_size,\n",
    "        sampler=SubsetRandomSampler(indices[split:]),\n",
    "        pin_memory=True, num_workers=6,persistent_workers=True\n",
    "    )\n",
    "\n",
    "    evaluator = Lightning(\n",
    "        AuxLossClassificationModule(0.0001, 3e-4, 0., 100),\n",
    "        Trainer(\n",
    "            accelerator=\"auto\",\n",
    "            max_epochs=100\n",
    "        ),\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=valid_loader\n",
    "    )\n",
    "\n",
    "    strategy = DartsStrategy(gradient_clip_val=5.)\n",
    "\n",
    "    experiment = NasExperiment(model_space, evaluator, strategy)\n",
    "    experiment.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746da731-e8bd-4410-bc15-af8d56e1a38b",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0c28cf6-e1ea-4c64-aed4-c45addbbbf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "def train(arch: dict, log_dir: str, batch_size: int = 96, ckpt_path: str = None):\n",
    "    \"\"\"\n",
    "    Train the model with the given architecture and parameters.\n",
    "\n",
    "    Args:\n",
    "        arch (dict): The architecture of the model.\n",
    "        log_dir (str): The directory where logs will be saved.\n",
    "        batch_size (int, optional): The size of the batches. Default is 96.\n",
    "        ckpt_path (str, optional): The path to the checkpoint file. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with model_context(arch):\n",
    "        model = DARTS(36, 20, 'cifar', auxiliary_loss=True, drop_path_prob=0.2)\n",
    "\n",
    "    train_data = get_cifar10_dataset(cutout=True)\n",
    "    valid_data = get_cifar10_dataset(train=False)\n",
    "\n",
    "    fit_kwargs = {}\n",
    "    if ckpt_path:\n",
    "        fit_kwargs['ckpt_path'] = ckpt_path\n",
    "\n",
    "    evaluator = Lightning(\n",
    "        AuxLossClassificationModule(0.025, 3e-4, 0.4, 600),\n",
    "        Trainer(\n",
    "            accelerator=\"auto\",\n",
    "            max_epochs=20\n",
    "        ),\n",
    "        train_dataloaders=DataLoader(train_data, batch_size=batch_size, pin_memory=True, shuffle=True, num_workers=6),\n",
    "        val_dataloaders=DataLoader(valid_data, batch_size=batch_size, pin_memory=True, num_workers=6),\n",
    "        fit_kwargs=fit_kwargs\n",
    "    )\n",
    "\n",
    "    evaluator.fit(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b168f-07b9-41c7-9282-00af6eaeadb8",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aa9625d2-ced5-44ad-a88c-646473a7efa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-03 00:07:04] \u001b[32mConfig is not provided. Will try to infer.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[32mStrategy is found to be a one-shot strategy. Setting execution engine to \"sequential\" and format to \"raw\".\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: Checkpoint callback does not have last_model_path or best_model_path attribute. Either the strategy has not started, or it did not save any checkpoint: <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x000002AA0915A4D0>\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[32mCheckpoint saved to C:\\Users\\senti\\nni-experiments\\5wgqdtc1\\checkpoint.\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[32mExperiment initialized successfully. Starting exploration strategy...\u001b[0m\n",
      "[2024-06-03 00:07:04] \u001b[33mWARNING: Validation dataloaders are missing. Safe to ignore this warning when using one-shot strategy.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                        | Params\n",
      "----------------------------------------------------------------\n",
      "0 | training_module | AuxLossClassificationModule | 30.9 K\n",
      "----------------------------------------------------------------\n",
      "30.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.9 K    Total params\n",
      "0.124     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  89%|████████▉ | 348/391 [00:11<00:01, 30.85it/s, v_num=30, train_loss=nan.0, train_acc=0.109] [2024-06-03 00:07:47] \u001b[33mWARNING: Trainer status is detected to be interrupted.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[31mERROR: Model 1 is interrupted. Exiting gracefully...\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: Strategy is interrupted.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: KeyboardInterrupt detected\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[32mStopping experiment, please wait...\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: Checkpoint callback does not have last_model_path or best_model_path attribute. Either the strategy has not started, or it did not save any checkpoint: <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x000002AA0915A4D0>\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[32mCheckpoint saved to C:\\Users\\senti\\nni-experiments\\5wgqdtc1\\checkpoint.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-06-03 00:07:47] \u001b[32mExperiment stopped\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "search(\"./\",64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89fdb6-d3b7-496d-b8da-8fbe10994231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b80062-67e3-4880-ad65-336b3498d2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e71719-571e-466b-a132-c0c0b2d15b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
