{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import tensorboard\n",
    "import tensorboardX\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nni\n",
    "from nni.nas.nn.pytorch import ModelSpace, LayerChoice, MutableConv2d, MutableBatchNorm2d, MutableReLU\n",
    "from pytorch_lightning import Trainer\n",
    "from nni.nas.evaluator.pytorch import Lightning, ClassificationModule, Trainer\n",
    "from nni.nas.experiment import NasExperiment\n",
    "from nni.nas.space import model_context\n",
    "from nni.nas.hub.pytorch import DARTS\n",
    "from nni.nas.strategy import DARTS as DartsStrategy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from nni.nas.experiment import NasExperiment\n",
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "import nni.nas.strategy as strategy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "#from ops import AvgPool,DilConv,SepConv\n",
    "import genotypes\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from tqdm import tqdm\n",
    "from nni.nas.nn.pytorch import LayerChoice, ModelSpace,ValueChoice\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchvision import datasets, transforms\n",
    "from nni.nas.evaluator.pytorch import Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nni.trace\n",
    "class AuxLossClassificationModule(ClassificationModule):\n",
    "    \"\"\"Several customization for the training of DARTS, based on default Classification.\"\"\"\n",
    "    model: DARTS\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 1e-5,\n",
    "                 weight_decay: float = 0.,\n",
    "                 auxiliary_loss_weight: float = 0.4,\n",
    "                 max_epochs: int = 600):\n",
    "        print(f\"lr : {learning_rate}\")\n",
    "        print(f\"weight decay: {weight_decay}\")\n",
    "        print(f\"aux loss weight: {auxiliary_loss_weight}\")\n",
    "        print(f\"max epochs: {max_epochs}\")\n",
    "        super().__init__(learning_rate=learning_rate, weight_decay=weight_decay, num_classes=10)\n",
    "        self.auxiliary_loss_weight = auxiliary_loss_weight\n",
    "        self.max_epochs = max_epochs\n",
    "        self.criterion=  nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Customized optimizer with momentum, as well as a scheduler.\"\"\"\n",
    "        \"\"\"\n",
    "        Classification Module params from nni.nas.evaluators\n",
    "             learning_rate: float\n",
    "             weight_decay: float\n",
    "             optimizer: Type[optim.Optimizer]\n",
    "             export_onnx: bool \n",
    "             num_classes: Optional[int] \n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr = 1e-6,\n",
    "            betas=(0.9, 0.999),  # type: ignore\n",
    "            eps=1e-07,\n",
    "            weight_decay= self.auxiliary_loss_weight  # type: ignore\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.max_epochs, eta_min=1e-3)\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step, customized with auxiliary loss.\"\"\"\n",
    "        x, y = batch\n",
    "\n",
    "        # Check for NaNs or infinite values in input\n",
    "        if torch.isnan(x).any() or torch.isnan(y).any() or torch.isinf(x).any() or torch.isinf(y).any():\n",
    "            raise ValueError(\"Input data contains NaNs or Infinities.\")\n",
    "\n",
    "        if self.auxiliary_loss_weight:\n",
    "            y_hat, y_aux = self(x)\n",
    "            loss_main = self.criterion(y_hat, y)\n",
    "            loss_aux = self.criterion(y_aux, y)\n",
    "            # Check for NaNs in loss values\n",
    "            if torch.isnan(loss_main).any() or torch.isnan(loss_aux).any():\n",
    "                raise ValueError(\"Loss contains NaNs.\")\n",
    "            self.log('train_loss_main', loss_main)\n",
    "            self.log('train_loss_aux', loss_aux)\n",
    "            loss = loss_main + self.auxiliary_loss_weight * loss_aux\n",
    "\n",
    "        else:\n",
    "            y_hat = self(x)\n",
    "            loss = self.criterion(y_hat, y)       \n",
    "        #acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        #self.log('train_accuracy', acc, prog_bar=True)  # Log training accuracy\n",
    "        for name, metric in self.metrics.items():\n",
    "            self.log('train_' + name, metric(y_hat, y), prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Set drop path probability before every epoch. This has no effect if drop path is not enabled in model.\"\"\"\n",
    "        self.model.set_drop_path_prob(self.model.drop_path_prob * self.current_epoch / self.max_epochs)\n",
    "\n",
    "        # Logging learning rate at the beginning of every epoch\n",
    "        self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/best-checkpoint-v1.ckpt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "model = AuxLossClassificationModule()\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "pretrained_state_dict = checkpoint['state_dict']\n",
    "\n",
    "filtered_state_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict}\n",
    "\n",
    "model_state_dict.update(filtered_state_dict)\n",
    "\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Print additional information from the checkpoint\n",
    "if 'epoch' in checkpoint:\n",
    "    print(\"Epoch:\", checkpoint['epoch'])\n",
    "\n",
    "if 'global_step' in checkpoint:\n",
    "    print(\"Global Step:\", checkpoint['global_step'])\n",
    "\n",
    "if 'callbacks' in checkpoint and isinstance(checkpoint['callbacks'], dict):\n",
    "    for key, callback in checkpoint['callbacks'].items():\n",
    "        if isinstance(callback, dict) and 'best_model_score' in callback:\n",
    "            print(\"Best Model Score (Train Accuracy):\", callback['best_model_score'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
