{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import tensorboard\n",
    "import tensorboardX\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nni\n",
    "from nni.nas.nn.pytorch import ModelSpace, LayerChoice, MutableConv2d, MutableBatchNorm2d, MutableReLU\n",
    "\n",
    "from nni.nas.evaluator.pytorch import Lightning, ClassificationModule, Trainer\n",
    "from nni.nas.experiment import NasExperiment\n",
    "from nni.nas.space import model_context\n",
    "from nni.nas.hub.pytorch import DARTS\n",
    "from nni.nas.strategy import DARTS as DartsStrategy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from nni.nas.experiment import NasExperiment\n",
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "import nni.nas.strategy as strategy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "#from ops import AvgPool,DilConv,SepConv\n",
    "import genotypes\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from tqdm import tqdm\n",
    "from nni.nas.nn.pytorch import LayerChoice, ModelSpace,ValueChoice\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchvision import datasets, transforms\n",
    "from nni.nas.evaluator.pytorch import Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Auxiliary Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nni.trace\n",
    "class AuxLossClassificationModule(ClassificationModule):\n",
    "    \"\"\"Several customization for the training of DARTS, based on default Classification.\"\"\"\n",
    "    model: DARTS\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 1e-5,\n",
    "                 weight_decay: float = 0.,\n",
    "                 auxiliary_loss_weight: float = 0.4,\n",
    "                 max_epochs: int = 600):\n",
    "        print(f\"lr : {learning_rate}\")\n",
    "        print(f\"weight decay: {weight_decay}\")\n",
    "        print(f\"aux loss weight: {auxiliary_loss_weight}\")\n",
    "        print(f\"max epochs: {max_epochs}\")\n",
    "        super().__init__(learning_rate=learning_rate, weight_decay=weight_decay, num_classes=10)\n",
    "        self.auxiliary_loss_weight = auxiliary_loss_weight\n",
    "        self.max_epochs = max_epochs\n",
    "        self.criterion=  nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Customized optimizer with momentum, as well as a scheduler.\"\"\"\n",
    "        \"\"\"\n",
    "        Classification Module params from nni.nas.evaluators\n",
    "             learning_rate: float\n",
    "             weight_decay: float\n",
    "             optimizer: Type[optim.Optimizer]\n",
    "             export_onnx: bool \n",
    "             num_classes: Optional[int] \n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr = 1e-6,\n",
    "            betas=(0.9, 0.999),  # type: ignore\n",
    "            eps=1e-07,\n",
    "            weight_decay= self.auxiliary_loss_weight  # type: ignore\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.max_epochs, eta_min=1e-3)\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step, customized with auxiliary loss.\"\"\"\n",
    "        x, y = batch\n",
    "\n",
    "        # Check for NaNs or infinite values in input\n",
    "        if torch.isnan(x).any() or torch.isnan(y).any() or torch.isinf(x).any() or torch.isinf(y).any():\n",
    "            raise ValueError(\"Input data contains NaNs or Infinities.\")\n",
    "\n",
    "        if self.auxiliary_loss_weight:\n",
    "            y_hat, y_aux = self(x)\n",
    "            loss_main = self.criterion(y_hat, y)\n",
    "            loss_aux = self.criterion(y_aux, y)\n",
    "            # Check for NaNs in loss values\n",
    "            if torch.isnan(loss_main).any() or torch.isnan(loss_aux).any():\n",
    "                raise ValueError(\"Loss contains NaNs.\")\n",
    "            self.log('train_loss_main', loss_main)\n",
    "            self.log('train_loss_aux', loss_aux)\n",
    "            loss = loss_main + self.auxiliary_loss_weight * loss_aux\n",
    "\n",
    "        else:\n",
    "            y_hat = self(x)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        for name, metric in self.metrics.items():\n",
    "            self.log('train_' + name, metric(y_hat, y), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Set drop path probability before every epoch. This has no effect if drop path is not enabled in model.\"\"\"\n",
    "        self.model.set_drop_path_prob(self.model.drop_path_prob * self.current_epoch / self.max_epochs)\n",
    "\n",
    "        # Logging learning rate at the beginning of every epoch\n",
    "        self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Random cutout transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout_transform(img, length: int = 16):\n",
    "    h, w = img.size(1), img.size(2)\n",
    "    mask = np.ones((h, w), np.float32)\n",
    "    y = np.random.randint(h)\n",
    "    x = np.random.randint(w)\n",
    "\n",
    "    y1 = np.clip(y - length // 2, 0, h)\n",
    "    y2 = np.clip(y + length // 2, 0, h)\n",
    "    x1 = np.clip(x - length // 2, 0, w)\n",
    "    x2 = np.clip(x + length // 2, 0, w)\n",
    "\n",
    "    mask[y1: y2, x1: x2] = 0.\n",
    "    mask = torch.from_numpy(mask)\n",
    "    mask = mask.expand_as(img)\n",
    "    img *= mask\n",
    "    return img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cifar10_dataset(train: bool = True, cutout: bool = False):\n",
    "    CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "    CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "    if train:\n",
    "        transform_list = [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "        ]\n",
    "        if cutout:\n",
    "            transform_list.append(cutout_transform)\n",
    "        transform = transforms.Compose(transform_list)\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "        ])\n",
    "\n",
    "    dataset = nni.trace(CIFAR10)(root='./data', train=train, download=True, transform=transform)\n",
    "    \n",
    "    # Check for NaNs in the dataset\n",
    "    # for img, label in tqdm(dataset, desc=\"Checking dataset for NaNs or Infinities\"):\n",
    "    #     if torch.isnan(img).any() or torch.isinf(img).any():\n",
    "    #         raise ValueError(\"Dataset contains NaNs or Infinities.\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Darts Model Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Model Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
    "        super(SepConv, self).__init__()\n",
    "        self.op = nn.Sequential(\n",
    "            MutableConv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            MutableConv2d(C_in, C_in, kernel_size=1, padding=0),\n",
    "            MutableConv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            MutableConv2d(C_in, C_out, kernel_size=1, padding=0),\n",
    "            MutableBatchNorm2d(C_out,affine=affine),\n",
    "            MutableReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "        \n",
    "                \n",
    "        \n",
    "class PhotonicSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        tmp = torch.exp((x - 0.145) / 0.073)\n",
    "        tmp = 1.005 + (0.06 - 1.005) / (1 + tmp)\n",
    "        return tmp.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model Kernel 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDARTSSpaceK2(ModelSpace):\n",
    "    def __init__(self, input_channels, channels, num_classes, layers):\n",
    "        super(CustomDARTSSpaceK2, self).__init__()\n",
    "        #self.first_iter = True\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.drop_path_prob = 0.0  \n",
    "        self.preliminary_layer = nn.Conv2d(3, 8, kernel_size=2, padding=0, bias=False)\n",
    "        \n",
    "        layer1 = LayerChoice([\n",
    "            SepConv(8, ValueChoice([12, 14, 16]), kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(8, ValueChoice([12, 14, 16]), kernel_size=1)\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(8, ValueChoice([12, 14, 16]), kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
    "            ) \n",
    "        ], label='layer_1')\n",
    "        self.layers.append(layer1)\n",
    "        \n",
    "        layer2 = LayerChoice([\n",
    "            SepConv(ValueChoice([12, 14, 16]), ValueChoice([16, 18, 20]), kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(ValueChoice([12, 14, 16]), ValueChoice([16, 18, 20]), kernel_size=1)\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(ValueChoice([12, 14, 16]), ValueChoice([16, 18, 20]), kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
    "            )         \n",
    "        ], label='layer_2')\n",
    "        self.layers.append(layer2)\n",
    "        \n",
    "        layer3 = LayerChoice([\n",
    "            SepConv(ValueChoice([16, 18, 20]), ValueChoice([20, 22, 24]), kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(ValueChoice([16, 18, 20]), ValueChoice([20, 22, 24]), kernel_size=1)\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(ValueChoice([16, 18, 20]), ValueChoice([20, 22, 24]), kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
    "            )              \n",
    "        ], label='layer_3')\n",
    "        self.layers.append(layer3)\n",
    "        \n",
    "        layer4 = LayerChoice([\n",
    "            SepConv(ValueChoice([20, 22, 24]), ValueChoice([28, 30, 32]), kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(ValueChoice([20, 22, 24]), ValueChoice([28, 30, 32]), kernel_size=1)\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(ValueChoice([20, 22, 24]), ValueChoice([28, 30, 32]), kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
    "            )             \n",
    "        ], label='layer_4')\n",
    "        self.layers.append(layer4)\n",
    "        \n",
    "        layer5 = LayerChoice([\n",
    "            SepConv(ValueChoice([28, 30, 32]), 40, kernel_size=3, stride=1, padding=0),   \n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0),\n",
    "                nn.Conv2d(ValueChoice([28, 30, 32]), 40, kernel_size=1)\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(ValueChoice([28, 30, 32]), 40, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
    "            )               \n",
    "        ], label='layer_5')\n",
    "        self.layers.append(layer5)\n",
    "        \n",
    "        # Ensure the number of inputs to fc1 is close to but not exceeding 200\n",
    "        self.pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(160, 96)  \n",
    "        self.fc2 = nn.Linear(96, 64)  \n",
    "        self.fc3 = nn.Linear(64, 32) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Kernel: 2\n",
    "        # Initial shape: 32x32\n",
    "        #if first_iter:print(f'Input shape: {x.shape}')\n",
    "        x = self.preliminary_layer(x)\n",
    "        # After preliminary layer: 31x31\n",
    "        #if first_iter:print(f'After preliminary layer: {x.shape}')\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            #if first_iter:print(f'After layer {i+1}: {x.shape}')\n",
    "            \n",
    "            # Add a AvgPool2d layer after the third layer to reduce spatial dimensions\n",
    "            if i == 2 or i == 4:\n",
    "                x = nn.AvgPool2d(kernel_size=2, stride=2)(x)\n",
    "                #if first_iter:print(f'After avg pooling: {x.shape}')\n",
    "        \n",
    "        # Add an adaptive pooling layer before flattening\n",
    "        x = self.pool(x)\n",
    "        #if first_iter:print(f'After adaptive pooling: {x.shape}')\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        #if first_iter:print(f'After flattening: {x.shape}')\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x= self.relu(x)\n",
    "        #if first_iter:print(f'After fc1: {x.shape}')\n",
    "        x = self.fc2(x)\n",
    "        x= self.relu(x)\n",
    "        #if first_iter:print(f'After fc2: {x.shape}')\n",
    "        x = self.fc3(x)\n",
    "        x= self.relu(x)\n",
    "        #if first_iter:print(f'After fc3: {x.shape}')\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        #if first_iter:print(f'After classifier: {x.shape}')\n",
    "        #self.first_iter = False\n",
    "        return x\n",
    "\n",
    "    def set_drop_path_prob(self, drop_path_prob):\n",
    "        self.drop_path_prob = drop_path_prob\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'set_drop_path_prob'):\n",
    "                layer.set_drop_path_prob(drop_path_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Model kernel 3x3\n",
    "avg pool dopo conv2d come scelta\n",
    "aggiungere sepconv con sigmoid in scelte\n",
    "aumentare numero conv dentro la sepconv\n",
    "add model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5a328-9978-4100-ba9a-ba3a0e6a0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "  SepConv(8, nni.choice([12, 14, 16], label='layer1_out'), kernel_size=3, stride=1, padding=1),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDARTSSpaceK3(ModelSpace):\n",
    "    def __init__(self, input_channels, channels, num_classes, layers, verbose):\n",
    "        super(CustomDARTSSpaceK3, self).__init__()\n",
    "        #self.first_iter = True\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.drop_path_prob = 0.0  \n",
    "        self.preliminary_layer = nn.Conv2d(3, 8, kernel_size=3, padding=0, bias=False)\n",
    "        self.verbose = verbose\n",
    "\n",
    " # Declare the choices using the correct format\n",
    "        layer1_out = nni.choice('layer1_out', [12, 14, 16])\n",
    "        layer2_in = nni.choice('layer2_in', [12, 14, 16])\n",
    "        layer2_out = nni.choice('layer2_out', [16, 18, 20])\n",
    "        layer3_in = nni.choice('layer3_in', [16, 18, 20])\n",
    "        layer3_out = nni.choice('layer3_out', [20, 22, 24])\n",
    "        layer4_in = nni.choice('layer4_in', [20, 22, 24])\n",
    "        layer4_out = nni.choice('layer4_out', [28, 30, 32])\n",
    "        layer5_in = nni.choice('layer5_in', [28, 30, 32])\n",
    "\n",
    "        # Use the declared choices within LayerChoice\n",
    "        layer1 = LayerChoice([\n",
    "              SepConv(8, layer1_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(8, layer1_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer1_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(8, layer1_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer1_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_1')\n",
    "        self.layers.append(layer1)\n",
    "        \n",
    "        layer2 = LayerChoice([\n",
    "            SepConv(layer2_in, layer2_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(layer2_in, layer2_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer2_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(layer2_in, layer2_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer2_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_2')\n",
    "        self.layers.append(layer2)\n",
    "        \n",
    "        layer3 = LayerChoice([\n",
    "            SepConv(layer3_in, layer3_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(layer3_in, layer3_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer3_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(layer3_in, layer3_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer3_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_3')\n",
    "        self.layers.append(layer3)\n",
    "        \n",
    "        layer4 = LayerChoice([\n",
    "            SepConv(layer4_in, layer4_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(layer4_in, layer4_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer4_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(layer4_in, layer4_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer4_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_4')\n",
    "        self.layers.append(layer4)\n",
    "        \n",
    "        layer5 = LayerChoice([\n",
    "            SepConv(layer5_in, 48, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(layer5_in, 48, kernel_size=1),\n",
    "                MutableBatchNorm2d(48),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(layer5_in, 48, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(48),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_5')\n",
    "        self.layers.append(layer5)\n",
    "        \n",
    "        # Ensure the number of inputs to fc1 is close to but not exceeding 200\n",
    "        self.pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(192, 128) \n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.fc3 = nn.Linear(64, 32)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Kernel: 2\n",
    "        # Initial shape: 32x32\n",
    "        #print(f'Input shape: {x.shape}')\n",
    "        x = self.preliminary_layer(x)\n",
    "        # After preliminary layer: 31x31\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After preliminary layer: {x.shape}')\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if self.verbose == 1 :\n",
    "                print(f'After layer {i+1}: {x.shape}')\n",
    "            \n",
    "            # Add a AvgPool2d layer after the third layer to reduce spatial dimensions\n",
    "            if i == 2 or i == 4:\n",
    "                x = nn.AvgPool2d(kernel_size=2, stride=2)(x)\n",
    "                if self.verbose == 1 :\n",
    "                    print(f'After avg pooling: {x.shape}')\n",
    "        \n",
    "\n",
    "        x =  nn.AvgPool2d(kernel_size=2, stride =2)(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After adaptive pooling: {x.shape}')\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After flattening: {x.shape}')\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x= self.relu(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After fc1: {x.shape}')\n",
    "        x = self.fc2(x)\n",
    "        x= self.relu(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After fc2: {x.shape}')\n",
    "        x = self.fc3(x)\n",
    "        x= self.relu(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After fc3: {x.shape}')\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After classifier: {x.shape}')\n",
    "        #self.first_iter = False\n",
    "        return x\n",
    "\n",
    "    def set_drop_path_prob(self, drop_path_prob):\n",
    "        self.drop_path_prob = drop_path_prob\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'set_drop_path_prob'):\n",
    "                layer.set_drop_path_prob(drop_path_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Darts Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(log_dir: str, batch_size: int = 64):\n",
    "    \"\"\"\n",
    "    Darts search \n",
    "\n",
    "    Args:\n",
    "        log_dir (str): The directory where logs will be saved.\n",
    "        batch_size (int, optional): The size of the batches. Default is 64.  \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    model_space =CustomDARTSSpaceK3(input_channels=3, channels=64, num_classes=10, layers=5)\n",
    "    model_space.set_drop_path_prob(0.2)\n",
    "    train_data = get_cifar10_dataset()\n",
    "    num_samples = len(train_data)\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    split = num_samples // 2\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=batch_size,\n",
    "        sampler=SubsetRandomSampler(indices[:split]),\n",
    "        pin_memory=True, num_workers=6,persistent_workers=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        train_data, batch_size=batch_size,\n",
    "        sampler=SubsetRandomSampler(indices[split:]),\n",
    "        pin_memory=True, num_workers=6,persistent_workers=True\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_accuracy',  \n",
    "        dirpath='./checkpoints', \n",
    "        filename='best-checkpoint',  \n",
    "        save_top_k=1,\n",
    "        mode='max'  \n",
    "    )\n",
    "\n",
    "\n",
    "    evaluator = Lightning(\n",
    "        AuxLossClassificationModule(1e-6, 3e-4, 0., 600),\n",
    "        Trainer(\n",
    "            accelerator=\"auto\",\n",
    "            callbacks=[checkpoint_callback],  \n",
    "            max_epochs=600\n",
    "        ),\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=valid_loader\n",
    "    )\n",
    "\n",
    "    strategy = DartsStrategy(gradient_clip_val=0.3)\n",
    "\n",
    "    experiment = NasExperiment(model_space, evaluator, strategy)\n",
    "    experiment.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "def train(arch: dict, log_dir: str, batch_size: int = 96, ckpt_path: str = None):\n",
    "    \"\"\"\n",
    "    Train the model with the given architecture and parameters.\n",
    "\n",
    "    Args:\n",
    "        arch (dict): The architecture of the model.\n",
    "        log_dir (str): The directory where logs will be saved.\n",
    "        batch_size (int, optional): The size of the batches. Default is 96.\n",
    "        ckpt_path (str, optional): The path to the checkpoint file. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with model_context(arch):\n",
    "        model = DARTS(36, 20, 'cifar', auxiliary_loss=True, drop_path_prob=0.2)\n",
    "\n",
    "    train_data = get_cifar10_dataset(cutout=True)\n",
    "    valid_data = get_cifar10_dataset(train=False)\n",
    "\n",
    "    fit_kwargs = {}\n",
    "    if ckpt_path:\n",
    "        fit_kwargs['ckpt_path'] = ckpt_path\n",
    "\n",
    "    evaluator = Lightning(\n",
    "        CustomClassificationModule(0.025, 3e-4, 0.4, 600),\n",
    "        Trainer(\n",
    "            \n",
    "            accelerator=\"auto\",\n",
    "            max_epochs=20\n",
    "        ),\n",
    "        train_dataloaders=DataLoader(train_data, batch_size=batch_size, pin_memory=True, shuffle=True, num_workers=6),\n",
    "        val_dataloaders=DataLoader(valid_data, batch_size=batch_size, pin_memory=True, num_workers=6),\n",
    "        fit_kwargs=fit_kwargs\n",
    "    )\n",
    "\n",
    "    evaluator.fit(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr : 1e-06\n",
      "weight decay: 0.0003\n",
      "aux loss weight: 0.0\n",
      "max epochs: 600\n",
      "[2024-07-23 13:02:42] \u001b[32mConfig is not provided. Will try to infer.\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[32mStrategy is found to be a one-shot strategy. Setting execution engine to \"sequential\" and format to \"raw\".\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:42] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:43] \u001b[33mWARNING: Checkpoint callback does not have last_model_path or best_model_path attribute. Either the strategy has not started, or it did not save any checkpoint: <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x00000220AB41BBD0>\u001b[0m\n",
      "[2024-07-23 13:02:43] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:43] \u001b[33mWARNING: `training_service` will be ignored for sequential execution engine.\u001b[0m\n",
      "[2024-07-23 13:02:43] \u001b[32mCheckpoint saved to C:\\Users\\Giovanni\\nni-experiments\\wynh2dzu\\checkpoint.\u001b[0m\n",
      "[2024-07-23 13:02:43] \u001b[32mExperiment initialized successfully. Starting exploration strategy...\u001b[0m\n",
      "[2024-07-23 13:02:43] \u001b[33mWARNING: Validation dataloaders are missing. Safe to ignore this warning when using one-shot strategy.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                        | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | training_module | AuxLossClassificationModule | 91.0 K | train\n",
      "------------------------------------------------------------------------\n",
      "91.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "91.0 K    Total params\n",
      "0.364     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/196 [00:00<?, ?it/s] [2024-07-23 13:03:34] \u001b[31mERROR: Model 1 fails to be executed.\u001b[0m\n",
      "[2024-07-23 13:03:34] \u001b[31mERROR: Strategy failed to execute.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x432 and 192x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m search(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m128\u001b[39m)\n",
      "Cell \u001b[1;32mIn[99], line 53\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(log_dir, batch_size)\u001b[0m\n\u001b[0;32m     50\u001b[0m strategy \u001b[38;5;241m=\u001b[39m DartsStrategy(gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m     52\u001b[0m experiment \u001b[38;5;241m=\u001b[39m NasExperiment(model_space, evaluator, strategy)\n\u001b[1;32m---> 53\u001b[0m experiment\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\experiment\\experiment.py:236\u001b[0m, in \u001b[0;36mExperiment.run\u001b[1;34m(self, port, wait_completion, debug)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, port: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8080\u001b[39m, wait_completion: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, debug: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    Run the experiment.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m    Otherwise, return ``True`` when experiment done; or return ``False`` when experiment failed.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_impl(port, wait_completion, debug)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\experiment\\experiment.py:205\u001b[0m, in \u001b[0;36mExperiment._run_impl\u001b[1;34m(self, port, wait_completion, debug)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, port: \u001b[38;5;28mint\u001b[39m, wait_completion: \u001b[38;5;28mbool\u001b[39m, debug: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart(port, debug)\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m wait_completion:\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_completion()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\experiment\\experiment.py:270\u001b[0m, in \u001b[0;36mNasExperiment.start\u001b[1;34m(self, port, debug, run_mode)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_without_nni_manager()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_engine_and_strategy()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\experiment\\experiment.py:230\u001b[0m, in \u001b[0;36mNasExperiment._start_engine_and_strategy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_checkpoint()\n\u001b[0;32m    228\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExperiment initialized successfully. Starting exploration strategy...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\strategy\\base.py:170\u001b[0m, in \u001b[0;36mStrategy.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_status \u001b[38;5;241m=\u001b[39m StrategyStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Explore the model space.\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Strategy doesn't wait for the models it submitted.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrategy has successfully finished.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\oneshot\\pytorch\\strategy.py:335\u001b[0m, in \u001b[0;36mOneShotStrategy._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutated_model_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOne-shot strategy is not initialized yet.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39msubmit_models(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutated_model_space)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\execution\\sequential.py:141\u001b[0m, in \u001b[0;36mSequentialExecutionEngine.submit_models\u001b[1;34m(self, *models)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    139\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning model \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_count, model)\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_model(model)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\execution\\sequential.py:88\u001b[0m, in \u001b[0;36mSequentialExecutionEngine._run_single_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m parameters \u001b[38;5;129;01mis\u001b[39;00m model\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Run training.\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m model\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Training success.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m status \u001b[38;5;241m=\u001b[39m ModelStatus\u001b[38;5;241m.\u001b[39mTrained\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\space\\space.py:132\u001b[0m, in \u001b[0;36mExecutableModelSpace.execute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluator is not set, but default execute requires an evaluator.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39m_execute(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\evaluator\\evaluator.py:145\u001b[0m, in \u001b[0;36mEvaluator._execute\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Advanced users can overwrite this to avoid instantiation of the deep learning model.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mFor internal uses only.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m executable_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mexecutable_model()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(executable_model)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\evaluator\\pytorch\\lightning.py:177\u001b[0m, in \u001b[0;36mLightning.evaluate\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloaders \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation dataloaders are missing. Safe to ignore this warning when using one-shot strategy.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_kwargs)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_interrupt:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerStatus\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    545\u001b[0m )\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    575\u001b[0m     ckpt_path,\n\u001b[0;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m )\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:252\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    250\u001b[0m             batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_optimization\u001b[38;5;241m.\u001b[39mrun(trainer\u001b[38;5;241m.\u001b[39moptimizers[\u001b[38;5;241m0\u001b[39m], batch_idx, kwargs)\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m             batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:94\u001b[0m, in \u001b[0;36m_ManualOptimization.run\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_run_start()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mStopIteration\u001b[39;00m):  \u001b[38;5;66;03m# no loop to break at this level\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(kwargs)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_run_end()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:114\u001b[0m, in \u001b[0;36m_ManualOptimization.advance\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kwargs  \u001b[38;5;66;03m# release the batch from memory\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\oneshot\\pytorch\\differentiable.py:64\u001b[0m, in \u001b[0;36mDartsLightningModule.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresample()\n\u001b[0;32m     63\u001b[0m arc_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 64\u001b[0m arc_step_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_module\u001b[38;5;241m.\u001b[39mtraining_step(val_batch, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m batch_idx)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arc_step_loss, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     66\u001b[0m     arc_step_loss \u001b[38;5;241m=\u001b[39m arc_step_loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[60], line 62\u001b[0m, in \u001b[0;36mAuxLossClassificationModule.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_main \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauxiliary_loss_weight \u001b[38;5;241m*\u001b[39m loss_aux\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[0;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(y_hat, y)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\nni\\nas\\evaluator\\pytorch\\lightning.py:243\u001b[0m, in \u001b[0;36mSupervisedLearningModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 243\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[98], line 138\u001b[0m, in \u001b[0;36mCustomDARTSSpaceK3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    135\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m#print(f'After flattening: {x.shape}')\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[0;32m    139\u001b[0m x\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m#print(f'After fc1: {x.shape}')\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\photonic\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x432 and 192x128)"
     ]
    }
   ],
   "source": [
    "search(\"./\",128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
