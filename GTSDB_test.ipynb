{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import pathlib\n",
    "import random as rn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "import argparse\n",
    "import json\n",
    "import tensorboard\n",
    "import tensorboardX\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import nni\n",
    "from nni.nas.nn.pytorch import ModelSpace, LayerChoice, MutableConv2d, MutableBatchNorm2d, MutableReLU\n",
    "from pytorch_lightning import Trainer\n",
    "from nni.nas.evaluator.pytorch import Lightning, ClassificationModule, Trainer\n",
    "from nni.nas.experiment import NasExperiment\n",
    "from nni.nas.space import model_context\n",
    "from nni.nas.hub.pytorch import DARTS\n",
    "from nni.nas.strategy import DARTS as DartsStrategy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from nni.nas.experiment import NasExperiment\n",
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "import nni.nas.strategy as strategy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import genotypes\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from tqdm import tqdm\n",
    "from nni.nas.nn.pytorch import LayerChoice, ModelSpace,ValueChoice\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchvision import datasets, transforms\n",
    "from nni.nas.evaluator.pytorch import Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(img):\n",
    "    #print(type(img), img.shape)\n",
    "    img1=np.uint8(cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX))\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img = clahe.apply(img1)\n",
    "    img = cv2.equalizeHist(img)\n",
    "    img = tf.expand_dims(img, axis=-1)\n",
    "    img = np.array(img)\n",
    "    img = img.astype(np.float64)\n",
    "    #print(type(img), img.shape)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/senti/Desktop/GTSDB'\n",
    "train_path = 'C:/Users/senti/Desktop/GTSDB/Train'\n",
    "test_path = 'C:/Users/senti/Desktop/GTSDB/Test/'\n",
    "height = 32\n",
    "width = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms.functional import equalize\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  \n",
    "    transforms.RandomCrop(32, padding=5),\n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.RandomRotation(15), \n",
    "    transforms.Lambda(equalize), \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to match training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(root=train_path, transform=val_transform)\n",
    "\n",
    "\n",
    "print(f\"Class to index mapping: {train_dataset.class_to_idx}\")\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "print(f\"Images shape: {images.shape}\") \n",
    "print(f\"Labels shape: {labels.shape}\")  \n",
    "print(f\"Labels: {labels}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}, Labels: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train dataset\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "# For validation dataset\n",
    "val_labels = [label for _, label in val_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_set = set(train_labels)\n",
    "unique_list = (list(list_set))\n",
    "\n",
    "print(unique_list)\n",
    "unique_list.append(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "fig, ax = plt.subplots(3, 4)\n",
    "fig.set_size_inches(16, 12)\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        img = images[i * 4 + j]\n",
    "        label = labels[i * 4 + j].item() \n",
    "        \n",
    "        ax[i, j].imshow(img.permute(1, 2, 0), cmap='gray') \n",
    "        ax[i, j].set_title(f\"Label: {classes[label]}\") \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nni.trace\n",
    "class AuxLossClassificationModule(ClassificationModule):\n",
    "    \"\"\"Several customization for the training of DARTS, based on default Classification.\"\"\"\n",
    "    model: DARTS\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 weight_decay: float = 0.,\n",
    "                 auxiliary_loss_weight: float = 0.4,\n",
    "                 max_epochs: int = 600):\n",
    "        print(f\"lr : {learning_rate}\")\n",
    "        print(f\"weight decay: {weight_decay}\")\n",
    "        print(f\"aux loss weight: {auxiliary_loss_weight}\")\n",
    "        print(f\"max epochs: {max_epochs}\")\n",
    "        super().__init__(learning_rate=learning_rate, weight_decay=weight_decay, num_classes=43)\n",
    "        self.auxiliary_loss_weight = auxiliary_loss_weight\n",
    "        self.max_epochs = max_epochs\n",
    "        self.criterion=  nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Customized optimizer with momentum, as well as a scheduler.\"\"\"\n",
    "        \"\"\"\n",
    "        Classification Module params from nni.nas.evaluators\n",
    "             learning_rate: float\n",
    "             weight_decay: float\n",
    "             optimizer: Type[optim.Optimizer]\n",
    "             export_onnx: bool \n",
    "             num_classes: Optional[int] \n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr = 1e-6,\n",
    "            betas=(0.9, 0.999), \n",
    "            eps=1e-07,\n",
    "            weight_decay= self.auxiliary_loss_weight \n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.max_epochs, eta_min=1e-3)\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step, customized with auxiliary loss.\"\"\"\n",
    "        x, y = batch\n",
    "\n",
    "        # Check for NaNs or infinite values in input\n",
    "        if torch.isnan(x).any() or torch.isnan(y).any() or torch.isinf(x).any() or torch.isinf(y).any():\n",
    "            raise ValueError(\"Input data contains NaNs or Infinities.\")\n",
    "\n",
    "        if self.auxiliary_loss_weight:\n",
    "            y_hat, y_aux = self(x)\n",
    "            loss_main = self.criterion(y_hat, y)\n",
    "            loss_aux = self.criterion(y_aux, y)\n",
    "            # Check for NaNs in loss values\n",
    "            if torch.isnan(loss_main).any() or torch.isnan(loss_aux).any():\n",
    "                raise ValueError(\"Loss contains NaNs.\")\n",
    "            self.log('train_loss_main', loss_main)\n",
    "            self.log('train_loss_aux', loss_aux)\n",
    "            loss = loss_main + self.auxiliary_loss_weight * loss_aux\n",
    "\n",
    "        else:\n",
    "            y_hat = self(x)\n",
    "            loss = self.criterion(y_hat, y)       \n",
    "        #acc = (y_hat.argmax(dim=1) == y).float().mean()4 \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        #self.log('train_accuracy', acc, prog_bar=True)  # Log training accuracy\n",
    "        for name, metric in self.metrics.items():\n",
    "            self.log('train_' + name, metric(y_hat, y), prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Set drop path probability before every epoch. This has no effect if drop path is not enabled in model.\"\"\"\n",
    "        self.model.set_drop_path_prob(self.model.drop_path_prob * self.current_epoch / self.max_epochs)\n",
    "\n",
    "        # Logging learning rate at the beginning of every epoch\n",
    "        self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
    "        super(SepConv, self).__init__()\n",
    "        self.op = nn.Sequential(\n",
    "            MutableConv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            MutableConv2d(C_in, C_in, kernel_size=1, padding=0),\n",
    "            MutableConv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            MutableConv2d(C_in, C_out, kernel_size=1, padding=0),\n",
    "            MutableBatchNorm2d(C_out,affine=affine),\n",
    "            MutableReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "        \n",
    "                \n",
    "        \n",
    "class PhotonicSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        tmp = torch.exp((x - 0.145) / 0.073)\n",
    "        tmp = 1.005 + (0.06 - 1.005) / (1 + tmp)\n",
    "        return tmp.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDARTSSpaceK3(ModelSpace):\n",
    "    def __init__(self, input_channels, channels, num_classes, layers, verbose):\n",
    "        super(CustomDARTSSpaceK3, self).__init__()\n",
    "        #self.first_iter = True\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.drop_path_prob = 0.0  \n",
    "        self.preliminary_layer = nn.Conv2d(3, 8, kernel_size=3, padding=0, bias=False)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        layer1_in = 8\n",
    "        layer1_out = nni.choice('layer1_out_channels', [2,4,8,16,32,64])\n",
    "        layer2_out= nni.choice('layer2_out_channels', [2,4,8,16,32,64])\n",
    "        layer3_out= nni.choice('layer3_out_channels', [2,4,8,16,32,64])\n",
    "        layer4_out= nni.choice('layer4_out_channels', [2,4,8,16,32,64])\n",
    "        layer5_out = 22 \n",
    "\n",
    "        \n",
    "        layer1 = LayerChoice([\n",
    "              SepConv(8, layer1_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(8, layer1_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer1_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(8, layer1_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer1_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_1')\n",
    "        self.layers.append(layer1)\n",
    "        \n",
    "        layer2 = LayerChoice([\n",
    "            SepConv(layer1_out, layer2_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(layer1_out, layer2_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer2_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(layer1_out, layer2_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer2_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_2')\n",
    "        self.layers.append(layer2)\n",
    "        \n",
    "        layer3 = LayerChoice([\n",
    "            SepConv(layer2_out, layer3_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(layer2_out, layer3_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer3_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(layer2_out, layer3_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer3_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_3')\n",
    "        self.layers.append(layer3)\n",
    "        \n",
    "        layer4 = LayerChoice([\n",
    "            SepConv(layer3_out, layer4_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(layer3_out, layer4_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer4_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(layer3_out, layer4_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer4_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_4')\n",
    "        self.layers.append(layer4)\n",
    "        \n",
    "        layer5 = LayerChoice([\n",
    "            SepConv(layer4_out, layer5_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableConv2d(layer4_out, layer5_out, kernel_size=1),\n",
    "                MutableBatchNorm2d(layer5_out),\n",
    "                MutableReLU()\n",
    "            ),   \n",
    "            nn.Sequential(\n",
    "                MutableConv2d(layer4_out, layer5_out, kernel_size=1),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                MutableBatchNorm2d(layer5_out),\n",
    "                MutableReLU()\n",
    "            )\n",
    "        ], label='layer_5')\n",
    "        self.layers.append(layer5)\n",
    "        \n",
    "        # Ensure the number of inputs to fc1 is close to but not exceeding 200\n",
    "        self.pool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(198, 128) \n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.fc3 = nn.Linear(64, 32)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Kernel: 3\n",
    "        # Initial shape: 32x32\n",
    "        #print(f'Input shape: {x.shape}')\n",
    "        x = self.preliminary_layer(x)\n",
    "        # After preliminary layer: 31x31\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After preliminary layer: {x.shape}')\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if self.verbose == 1 :\n",
    "                print(f'After layer {i+1}: {x.shape}')\n",
    "            \n",
    "            # Add a AvgPool2d layer after the third layer to reduce spatial dimensions\n",
    "            if i == 2 or i == 4:\n",
    "                x = nn.AvgPool2d(kernel_size=2, stride=2)(x)\n",
    "                if self.verbose == 1 :\n",
    "                    print(f'After avg pooling: {x.shape}')\n",
    "        \n",
    "\n",
    "        x =  nn.AvgPool2d(kernel_size=2, stride =2)(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After adaptive pooling: {x.shape}')\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After flattening: {x.shape}')\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x= self.relu(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After fc1: {x.shape}')\n",
    "        x = self.fc2(x)\n",
    "        x= self.relu(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After fc2: {x.shape}')\n",
    "        x = self.fc3(x)\n",
    "        x= self.relu(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After fc3: {x.shape}')\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        if self.verbose == 1 :\n",
    "            print(f'After classifier: {x.shape}')\n",
    "        #self.first_iter = False\n",
    "        return x\n",
    "\n",
    "    def set_drop_path_prob(self, drop_path_prob):\n",
    "        self.drop_path_prob = drop_path_prob\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'set_drop_path_prob'):\n",
    "                layer.set_drop_path_prob(drop_path_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(log_dir: str, batch_size: int = 16):\n",
    "    \"\"\"\n",
    "    Darts search \n",
    "\n",
    "    Args:\n",
    "        log_dir (str): The directory where logs will be saved.\n",
    "        batch_size (int, optional): The size of the batches. Default is 64.  \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    model_space =CustomDARTSSpaceK3(input_channels=3, channels=64, num_classes=43, layers=5,verbose =0)\n",
    "    model_space.set_drop_path_prob(0.2)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='train_acc',  \n",
    "        dirpath='./checkpoints', \n",
    "        filename='best-checkpoint',  \n",
    "        save_top_k=1,\n",
    "        mode='max'  \n",
    "    )\n",
    "\n",
    "\n",
    "    evaluator = Lightning(\n",
    "        AuxLossClassificationModule(1e-3, 3e-4, 0., 600),\n",
    "        Trainer(\n",
    "            accelerator=\"auto\",\n",
    "            callbacks=[checkpoint_callback],  \n",
    "            max_epochs=600\n",
    "        ),\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=valid_loader\n",
    "    )\n",
    "\n",
    "    strategy = DartsStrategy(gradient_clip_val=0.3)\n",
    "\n",
    "    experiment = NasExperiment(model_space, evaluator, strategy)\n",
    "    experiment.run()\n",
    "    return experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = search(\"./\",16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_arch = experiment_results.export_top_models(formatter='dict')[0]\n",
    "\n",
    "exported_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/best-checkpoint-v9.ckpt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "model_checkpoint = CustomDARTSSpaceK3(input_channels=3, channels=64, num_classes=43, layers=5,verbose =0)\n",
    "\n",
    "model_state_dict = model_checkpoint.state_dict()\n",
    "pretrained_state_dict = checkpoint['state_dict']\n",
    "\n",
    "filtered_state_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict}\n",
    "\n",
    "model_state_dict.update(filtered_state_dict)\n",
    "\n",
    "model_checkpoint.load_state_dict(model_state_dict)\n",
    "\n",
    "torch.save(model_checkpoint,\"testmodel.pkt\")\n",
    "\n",
    "if 'global_step' in checkpoint:\n",
    "    print(\"Global Step:\", checkpoint['global_step'])\n",
    "\n",
    "if 'callbacks' in checkpoint and isinstance(checkpoint['callbacks'], dict):\n",
    "    for key, callback in checkpoint['callbacks'].items():\n",
    "        if isinstance(callback, dict) and 'best_model_score' in callback:\n",
    "            print(\"Best Model Score (Train Accuracy):\", callback['best_model_score'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_checkpoint = model_checkpoint.to(device) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_checkpoint.parameters(), lr=1e-2, weight_decay=3e-4)\n",
    "\n",
    "if 'optimizer_state_dict' in checkpoint:\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(checkpoint['optimizer_state_dict'])\n",
    "model_checkpoint.set_drop_path_prob(0.2)\n",
    "model_checkpoint.train()\n",
    "def continue_training(model, train_loader, criterion, optimizer, num_epochs, start_epoch=0):\n",
    "    top_acc = 0.0\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Check the range of labels before moving to device      \n",
    "            # Move inputs and labels to the same device as the model\n",
    "            inputs = inputs.to(device, dtype=torch.float32)  # Ensure inputs are of type float32\n",
    "            labels = labels.to(device, dtype=torch.long)  # Ensure labels are of type long\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Debug: Check shapes and values\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        if accuracy > top_acc:\n",
    "            top_acc = accuracy\n",
    "    print('Finished Training')\n",
    "\n",
    "continue_training(model_checkpoint, train_loader, criterion, optimizer, num_epochs=600, start_epoch=0)\n",
    "print(f'Top accuracy: {top_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, valid_loader, criterion):\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(valid_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    valid_loss = running_loss / len(valid_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Validation Loss: {valid_loss:.4f}, Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return valid_loss, accuracy\n",
    "\n",
    "valid_loss, valid_accuracy = evaluate_model(model_checkpoint, valid_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
